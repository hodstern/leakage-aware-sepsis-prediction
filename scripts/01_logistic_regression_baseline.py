# Baseline Logistic Regression for Sepsis Prediction (6h Horizon)
#
# The purpose is NOT performance optimization, but:
#   1. Leakage detection
#   2. Pipeline sanity checking
#   3. Establishing a defensible baseline
#
# ml_export_6h.csv is generated by the SQL pipeline in /sql/ and is NOT included in this repository due to MIMIC-IV data governance restrictions.

# Imports

import pandas as pd
import numpy as np

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, average_precision_score
from sklearn.calibration import calibration_curve
from sklearn.metrics import precision_recall_curve

import matplotlib.pyplot as plt

# Load data

df = pd.read_csv("ml_export_6h.csv")

print("Total rows:", len(df))
print(df["data_split"].value_counts())

# Train / validation / test split

train = df[df.data_split == "train"].copy()
val   = df[df.data_split == "val"].copy()
test  = df[df.data_split == "test"].copy()

# Separate features and label

TARGET = "label_sepsis_6h"
DROP_COLS = [
    "stay_id",
    "data_split",
    TARGET,
    "sepsis_onset_hour"
]

X_train = train.drop(columns=DROP_COLS)
y_train = train[TARGET]

X_val = val.drop(columns=DROP_COLS)
y_val = val[TARGET]

X_test = test.drop(columns=DROP_COLS)
y_test = test[TARGET]

print("Feature matrix shape:", X_train.shape)

# Convert PostgreSQL booleans ('t'/'f') to numeric (PostgreSQL booleans may appear as 't'/'f' strings after CSV export and need to be converted to 1/0 to allow numeric imputation)

def tf_to_numeric(df):
    df = df.copy()
    for col in df.columns:
        if df[col].dtype == object:
            unique_vals = set(df[col].dropna().unique())
            if unique_vals.issubset({"t", "f"}):
                df[col] = df[col].map({"t": 1, "f": 0})
    return df

X_train = tf_to_numeric(X_train)
X_val   = tf_to_numeric(X_val)
X_test  = tf_to_numeric(X_test)

# Missing value imputation with median

imputer = SimpleImputer(strategy="median")

X_train = imputer.fit_transform(X_train)
X_val   = imputer.transform(X_val)
X_test  = imputer.transform(X_test)

# Feature scaling

scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)
X_val   = scaler.transform(X_val)
X_test  = scaler.transform(X_test)

# Train logistic regression (no class weighting is applied since this is a pure baseline intended for leakage detection and pipeline validation)

clf = LogisticRegression(
    C=1.0,
    max_iter=1000,
    random_state=42
)

clf.fit(X_train, y_train)

# Evaluation

def evaluate(split_name, X, y):
    probs = clf.predict_proba(X)[:, 1]
    auroc = roc_auc_score(y, probs)
    auprc = average_precision_score(y, probs)
    print(f"{split_name:5s} | AUROC: {auroc:.3f} | AUPRC: {auprc:.3f}")

print("\nBaseline performance")
print("---------------------")
evaluate("train", X_train, y_train)
evaluate("val",   X_val,   y_val)
evaluate("test",  X_test,  y_test)

# Coefficient inspection (sanity check, not feature selection)

feature_names = train.drop(columns=DROP_COLS).columns
coef = pd.Series(clf.coef_[0], index=feature_names)

print("\nTop positive coefficients:")
print(coef.sort_values(ascending=False).head(10))

print("\nTop negative coefficients:")
print(coef.sort_values().head(10))

# Calibration curve (test set)

probs_test = clf.predict_proba(X_test)[:, 1]

fraction_of_positives, mean_predicted_value = calibration_curve(
    y_test,
    probs_test,
    n_bins=10,
    strategy="quantile"
)

plt.figure(figsize=(6, 6))
plt.plot(mean_predicted_value, fraction_of_positives, "o-", label="Logistic regression")
plt.plot([0, 1], [0, 1], "--", color="gray", label="Perfect calibration")

plt.xlabel("Mean predicted probability")
plt.ylabel("Observed sepsis frequency")
plt.title("Calibration curve – Sepsis prediction (6h horizon)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Precision–Recall curve and threshold analysis (more informative than ROC curves for imbalanced data and directly relevant for clinical alerting)

precision, recall, thresholds = precision_recall_curve(y_test, probs_test)

plt.figure(figsize=(6, 6))
plt.plot(recall, precision, label="Logistic regression")
plt.axhline(y=y_test.mean(), linestyle="--", color="gray", label="Baseline prevalence")

plt.xlabel("Recall (sensitivity)")
plt.ylabel("Precision (PPV)")
plt.title("Precision–Recall curve – Sepsis prediction (6h horizon)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Example decision thresholds

threshold_df = pd.DataFrame({
    "threshold": thresholds,
    "precision": precision[:-1],
    "recall": recall[:-1]
})

print("\nOperating points (approximate):")
print(threshold_df.iloc[(threshold_df.recall - 0.5).abs().argsort()[:1]])
print(threshold_df.iloc[(threshold_df.recall - 0.7).abs().argsort()[:1]])

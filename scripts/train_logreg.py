# Baseline Logistic Regression for Sepsis Prediction (6h Horizon)
#
# The purpose is NOT performance optimization, but:
#   1. Leakage detection
#   2. Pipeline sanity checking
#   3. Establishing a defensible baseline
#
# This model is intentionally simple and interpretable.
# It serves as the reference model for comparison with more complex approaches.
#
# The input file (ml_export_6h.csv) is generated by the SQL pipeline in /sql/
# and is NOT included in this repository due to MIMIC-IV data governance restrictions.

# Imports

from pathlib import Path
import json

import pandas as pd
import numpy as np

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, average_precision_score

# Paths

try:
    BASE_DIR = Path(__file__).resolve().parents[1]
except NameError:
    BASE_DIR = Path.cwd().resolve()

DATA_FILE = BASE_DIR / "ml_export_6h.csv"
RESULTS_DIR = BASE_DIR / "results"

RESULTS_DIR.mkdir(parents=True, exist_ok=True)

PRED_FILE = RESULTS_DIR / "predictions_logreg.csv"
METRICS_FILE = RESULTS_DIR / "metrics_logreg.json"

# Load data

df = pd.read_csv(DATA_FILE)

print("Total rows:", len(df))
print(df["data_split"].value_counts())

# Train / validation / test split

train = df[df.data_split == "train"].copy()
val   = df[df.data_split == "val"].copy()
test  = df[df.data_split == "test"].copy()

# Separate features and label

TARGET = "label_sepsis_6h"
DROP_COLS = [
    "stay_id",
    "data_split",
    TARGET,
    "sepsis_onset_hour",  # explicit leakage removal
]

X_train = train.drop(columns=DROP_COLS)
y_train = train[TARGET]

X_val = val.drop(columns=DROP_COLS)
y_val = val[TARGET]

X_test = test.drop(columns=DROP_COLS)
y_test = test[TARGET]

print("Feature matrix shape:", X_train.shape)

# Convert PostgreSQL booleans ('t'/'f') to numeric

def tf_to_numeric(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    for col in df.columns:
        if df[col].dtype == object:
            unique_vals = set(df[col].dropna().unique())
            if unique_vals.issubset({"t", "f"}):
                df[col] = df[col].map({"t": 1, "f": 0})
    return df

X_train = tf_to_numeric(X_train)
X_val   = tf_to_numeric(X_val)
X_test  = tf_to_numeric(X_test)

# Missing value imputation (median)

imputer = SimpleImputer(strategy="median")

X_train = imputer.fit_transform(X_train)
X_val   = imputer.transform(X_val)
X_test  = imputer.transform(X_test)

# Feature scaling

scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)
X_val   = scaler.transform(X_val)
X_test  = scaler.transform(X_test)

# Model definition and training

clf = LogisticRegression(
    C=1.0,
    max_iter=1000,
    random_state=42,
)

clf.fit(X_train, y_train)

# Evaluation

def evaluate_split(X, y):
    probs = clf.predict_proba(X)[:, 1]
    return {
        "auroc": roc_auc_score(y, probs),
        "auprc": average_precision_score(y, probs),
        "n": int(len(y)),
    }, probs


metrics = {}

metrics["train"], _ = evaluate_split(X_train, y_train)
metrics["val"],   _ = evaluate_split(X_val,   y_val)
metrics["test"],  probs_test = evaluate_split(X_test,  y_test)

metrics["prevalence"] = float(y_test.mean())

print("\nBaseline performance")
print("---------------------")
for split in ["train", "val", "test"]:
    m = metrics[split]
    print(f"{split:5s} | AUROC: {m['auroc']:.3f} | AUPRC: {m['auprc']:.3f}")

# Coefficient inspection (sanity check only)

feature_names = train.drop(columns=DROP_COLS).columns
coef = pd.Series(clf.coef_[0], index=feature_names)

print("\nTop positive coefficients:")
print(coef.sort_values(ascending=False).head(10))

print("\nTop negative coefficients:")
print(coef.sort_values().head(10))

# Save outputs

pd.DataFrame({
    "y_true": y_test,
    "y_pred": probs_test,
}).to_csv(PRED_FILE, index=False)

with open(METRICS_FILE, "w") as f:
    json.dump(metrics, f, indent=2)
